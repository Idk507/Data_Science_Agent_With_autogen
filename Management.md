
---

## ğŸš€ StepÂ 0: Project Kickoff & Setup  
1. **Create your repo & board**  
   - ForkÂ `AI_Agents_Hackathon` and create a feature branch (`feature/multiâ€‘agentâ€‘ds`).  
   - Stand up a lightweight Kanban board (GitHub Projects, Trello) with columns: Backlog, InÂ Progress, Review, Done.  
2. **Define global contracts & orchestrator stub**  
   - Draft a JSON schema for agent calls:  
     ```json
     { "agent": "string", "inputs": {â€¦}, "outputs": {â€¦}, "status": "pending|done|error" }
     ```  
   - Build an empty **Orchestrator Agent** skeleton (just log and forward calls).  
3. **Environment**  
   - Spin up a PythonÂ 3.10Â venv, install Autogen + Azure AI SDKs.  
   - Verify you can run `az login` and call a test OpenAI prompt.

---

## ğŸ—„ï¸ StepÂ 1: Data Ingestion Agent  
1. **Define Interface**  
   - Inputs: connection string / file path  
   - Outputs: raw DataFrame location (BlobÂ URI)  
2. **Implement MVP**  
   - Use pandas / Azure Data Factory SDK to fetch and save a CSV to Blob.  
   - Return a JSON with `{"blob_uri": "...", "row_count": 1234}`.  
3. **Unit Test**  
   - Mock a small CSV, assert blob upload and correct row count.  
4. **Integrate**  
   - Wire into Orchestrator: call ingestion, receive blob URI, pass to next agent.

---

## ğŸ“Š StepÂ 2: Data Profiling Agent  
1. **Define Interface**  
   - Inputs: `blob_uri`  
   - Outputs: profiling report JSON (column stats) + sample CSV.  
2. **Implement MVP**  
   - Use `pandas_profiling.ProfileReport` or AzureÂ ML DataPrep to generate summary.  
   - Serialize key stats (null %, dtypes, unique counts).  
3. **Test & Validate**  
   - Run on a known dataset; compare output to handâ€‘computed stats.  
4. **Integration**  
   - Hook profiling agent after ingestion; store its JSON in orchestratorâ€™s state.

---

## ğŸ“ˆ StepÂ 3: EDA & Visualization Agent  
1. **Define Interface**  
   - Inputs: profiling JSON + sample data  
   - Outputs: list of chart specs + generated PNG URLs + text insights.  
2. **Implement Chart Recommendation**  
   - Prompt GPTâ€‘4 (Autogen) with schema â†’ suggest `["histogram", "heatmap", â€¦]`.  
3. **Static Plot Agent**  
   - In a sandbox, execute matplotlib code to render each chart, upload to Blob, return URLs.  
4. **Insight Extraction**  
   - Prompt GPTâ€‘4 with chart stats â†’ generate short narrative (â€œSales peaked in Q4â€¦â€).  
5. **Test**  
   - Validate on a small dataset; ensure charts look correct and insights make sense.  
6. **Integrate**  
   - Connect to orchestrator; show sample chart + insight in logs/UI.

---

## ğŸ§¹ StepÂ 4: Data Cleaning Agent  
1. **Define Interface**  
   - Inputs: raw blob URI, profiling JSON  
   - Outputs: cleaned data blob URI + log of actions taken.  
2. **Implement MVP**  
   - Autoâ€‘impute numeric nulls (median), drop constant columns.  
   - Use Autogen prompts to suggest alternative strategies.  
3. **Test**  
   - Feed in synthetic data with known nulls/outliers; confirm cleaning.  
4. **Integrate**  
   - Place after EDA; pass cleaned data URI downstream.

---

## âš™ï¸ StepÂ 5: Feature Engineering Agent  
1. **Define Interface**  
   - Inputs: cleaned data URI  
   - Outputs: features dataset URI + feature metadata JSON  
2. **Implement**  
   - Oneâ€‘hot encode categoricals, scale numerics, generate dateâ€‘based features.  
   - Optionally embed text columns with Azure ML embeddings API.  
3. **Test**  
   - Run on toy data, inspect output shape and metadata.  
4. **Integrate**  
   - Update orchestrator to route cleaned data into this agent.

---

## ğŸ¤– StepÂ 6: Model Training Agent  
1. **Define Interface**  
   - Inputs: features URI + target column  
   - Outputs: model artifact URI + baseline metrics  
2. **Implement MVP**  
   - Train a simple scikitâ€‘learn model (e.g., RandomForest) in an Azure ML Pipeline.  
   - Log to Azure ML Experiment.  
3. **Test**  
   - Ensure model serializes, metrics match expected.  
4. **Integrate**  
   - Connect training agent; pass its artifact URI to the tuning agent.

---

## ğŸ” StepÂ 7: Hyperparameter Tuning Agent  
1. **Define Interface**  
   - Inputs: model artifact + parameter grid  
   - Outputs: best model URI + best params  
2. **Implement**  
   - Use Azure ML HyperDrive or Optuna in parallel runs.  
3. **Test**  
   - On a small grid, confirm the best trial is selected.  
4. **Integrate**  
   - Chain after baseline training agent.

---

## ğŸ“ StepÂ 8: Evaluation & Validation Agent  
1. **Define Interface**  
   - Inputs: best model URI + test set URI  
   - Outputs: metrics JSON + confusion/ROC plot URLs  
2. **Implement**  
   - Compute metrics (AUC, RMSE, etc.), generate and upload plots.  
3. **Test**  
   - Validate metrics on known splits.  
4. **Integrate**  
   - Slot in after tuning agent.

---

## ğŸ” StepÂ 9: Explainability Agent  
1. **Define Interface**  
   - Inputs: model URI + sample data  
   - Outputs: SHAP/LIME plot URLs + narrative JSON  
2. **Implement**  
   - Compute SHAP values, render plots, prompt GPTâ€‘4 for plainâ€‘English.  
3. **Test**  
   - Verify plot correctness and summary clarity.  
4. **Integrate**  
   - Connect to orchestrator to enrich final report.

---

## ğŸ“‘ StepÂ 10: Reporting & Dashboard Agent  
1. **Define Interface**  
   - Inputs: all prior outputs (charts, metrics, explanations)  
   - Outputs: HTML/Markdown report URI + PowerÂ BI dashboard link  
2. **Implement**  
   - Use Jinja2 to stitch narrative + visuals into a report.  
   - Optionally publish to PowerÂ BI via REST API.  
3. **Test**  
   - Generate sample report, review formatting and links.  
4. **Integrate**  
   - Final step of orchestratorâ€™s pipeline.

---

## ğŸš¢ StepÂ 11: Deployment & Monitoring Agent  
1. **Define Interface**  
   - Inputs: final model URI  
   - Outputs: endpoint URL + monitoring dashboard link  
2. **Implement**  
   - Containerize model as Azure Container Instance or Function.  
   - Hook up Application Insights for latency, error, drift alerts.  
3. **Test**  
   - Send test invokes, simulate drift, confirm alerts.  
4. **Integrate**  
   - Tie into orchestratorâ€™s â€œgoâ€‘liveâ€ command.

---

## ğŸ”„ StepÂ 12: Feedback & Knowledge Agent  
1. **Define Interface**  
   - Inputs: user feedback events + artifact metadata  
   - Outputs: retraining triggers + Q&A endpoint  
2. **Implement**  
   - Store feedback in CosmosÂ DB.  
   - Use Azure Cognitive Search + GPTâ€‘4 to answer â€œWhat was our last AUC?â€  
3. **Test**  
   - Simulate feedback, query the knowledge agent.  
4. **Integrate**  
   - Hook into postâ€‘deployment monitoring cycle.

---

## âœ… StepÂ 13: Endâ€‘toâ€‘End Integration & Release  
1. **Smoke Test**  
   - Run the full pipeline on a sample dataset.  
   - Fix any orchestration or dataâ€‘flow issues.  
2. **Documentation & Demo**  
   - Update `README.md` with setup, agent contracts, and usage.  
   - Capture a 2â€‘min demo GIF/video showing each agentâ€™s output.  
3. **Submit**  
   - Fill out the `project.yml` in the hackathon repo, linking code, report, and demo.  
   - Review in â€œReviewâ€ column, then move to â€œDone.â€

---
